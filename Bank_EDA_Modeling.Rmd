---
title: "Bank Analysis And Modeling"
author: "Taylor Bonar & Michael Burgess & Rashmi Patel"
date: "7/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # Data handling
library(naniar) # Viz on Missing Data
library(GGally) # Graphs!
library(DataExplorer) # More Graphs!
library(ggplot2) # More more Graphs!
library(cowplot) # ggplot2 defaults
library(funModeling) # Helpful Functions for EDA Function
library(Hmisc) # Helpful Functions for EDA Function
library(caret) # Data Partitioning
library(glmnet) # Modeling
library(coefplot) # Coefficient modeling of glmnet objects
library(corrplot) # Correlation Plotting
library(car) # vif function
library(pROC) # ROC Curves

setwd(".")
basic_eda <- function(data) # Sample Function Source: https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}

# logit^{-1}: Can use to convert results of logistic regression to probability
invlogit <- function(x) {1 / (1 + exp(-x))}

# Pseudo R^2

psuedo_rsq <- function(glmmodel) {
# Creating Psuedo R^2
full.ll.null <- glmmodel$null.deviance/-2
full.ll.proposed <- glmmodel$deviance/-2

full.ll.rsq <- (full.ll.null - full.ll.proposed) / full.ll.null

# Chi-square distribution p-value
pchisq <- 1 - pchisq(2*(full.ll.proposed - full.ll.null), df = (length(full.logistic.fit$coefficients)-1))
psuedo_rsq <- data.frame(
  psuedo_rsq=full.ll.rsq,
  chisq_pval=pchisq)

print(psuedo_rsq)
}

s_plot <- function(glmmodel,bank_data){
  predicted.data <- data.frame(
  probability.of.subscribe=glmmodel$fitted.values,
  bank.term.deposit=bank_data$y)
 
  predicted.data <- predicted.data[
    order(predicted.data$probability.of.subscribe, decreasing=FALSE),]
  predicted.data$rank <- 1:nrow(predicted.data)

## Lastly, we can plot the predicted probabilities for each sample subscribing
## and color by whether or not they actually subscribed
ggplot(data=predicted.data, aes(x=rank, y=probability.of.subscribe)) +
  geom_point(aes(color=bank.term.deposit), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of Subscribing")
}

get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}
```
# Introduction
The goal of this paper is to investigate banking data via an exploratory data analysis. Once we have initially examined the data, we will then move forward with attempting a classification model via logistic regression for predicting whether or not a client will subscribe to a banking institution given a direct marketing campaign taking place.

# Exploratory Data Analysis

The data we'll be exploring is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) relating to the direct marketing campaigns of a Portuguese banking institution. The initial data set consists of 41,188 observations with 20 inputs, dating between May 2008 to November 2010.

As described by the UCI Machine Learning Repository, each of the variables/columns are described as:

>**Input variables:**
>**bank client data:**
>
>1 - age (numeric)
>
>2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
>
>3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
>
>4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
>
>5 - default: has credit in default? (categorical: 'no','yes','unknown')
>
>6 - housing: has housing loan? (categorical: 'no','yes','unknown')
>
>7 - loan: has personal loan? (categorical: 'no','yes','unknown')
>
>**related with the last contact of the current campaign:**
>
>8 - contact: contact communication type (categorical: 'cellular','telephone')
>
>9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
>
>10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
>
>11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
>
>**other attributes:**
>
>12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
>
>13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
>
>14 - previous: number of contacts performed before this campaign and for this client (numeric)
>
>15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
>
># social and economic context attributes
>
>16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
>
>17 - cons.price.idx: consumer price index - monthly indicator (numeric)
>
>18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
>
>19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
>
>20 - nr.employed: number of employees - quarterly indicator (numeric)
>
>**Output variable (desired target):**
>
>21 - y - has the client subscribed a term deposit? (binary: 'yes','no')

Another caution we will need to access is that within this dataset, there is potential for more than one contact to the same client. This repeat contact was necessary as it was required to assess the product (i.e. a bank term deposit) and whether the client would or would not be subscribed.

```{r EDA}
# Retrieve datasets zip
if(!file.exists("./data/bank.zip")) {
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", "./bank-additional.zip", mode="wb")
}
unzip("./bank-additional.zip", files = c("bank-additional/bank-additional.csv","bank-additional/bank-additional-full.csv","bank-additional/bank-additional-names.txt"))

# Read data into a data frame object
full_bank <- read.csv("./bank-additional/bank-additional-full.csv", header = T, sep = ";")

# Use function to create initial data insights for bank data
basic_eda(full_bank)

# Create a bird's eye view of missing data using naniar library if missing data exists
if(sum(!complete.cases(full_bank)) > 0)
{
  vis_miss(full_bank, cluster = F) + # Without aggregating observations
  labs(title = "NAs in Bank Data from May 2008 - Nov 2010") +
  theme(axis.text.x = element_text(angle=90))
}
```

As we can see in the initial EDA, we have a small percentage of data that is marked unknown. As we cannot accurately categorize this data into their appropriate categories, we'll convert this data into NAs and drop them since they are at most 1.5% of the total data.
```{r unknown-transformation}
full_bank[full_bank=="unknown"] <- NA

freq(full_bank, plot = F) 

# Create a bird's eye view of missing data using naniar library if missing data exists
vis_miss(full_bank, cluster = F) +
labs(title = "NAs in Bank Data from May 2008 - Nov 2010") +
theme(axis.text.x = element_text(angle=90))

gg_miss_upset(full_bank)
```
Some interesting notes on the missing data is that the majority of the data seems to be the unknown status of whether an individual has defaulted on their credit or not. If default becomes a major predictor for whether a client will subscribe to a bank term deposit or not, some research should be explored here to discover why we cannot accurately define whether an individual defaults on their credit or not.

```{r remove-NAs}
complete_full_bank <- full_bank[complete.cases(full_bank),]

# Transform all chr objects in data frame to a factor class as a secondary data frame object
complete_full_bank <- as.data.frame(unclass(complete_full_bank), stringsAsFactors = T)

vis_miss(complete_full_bank, cluster = F) + # Without aggregating observations b/c takes too long to aggregate
labs(title = "Cleaned Observations in Bank Data from May 2008 - Nov 2010") +
theme(axis.text.x = element_text(angle=90))
```
Now we have no missing data.

## Addressing Character Variable and Numeric Variable
```{r} 
# Checking for number of columns with numeric type
numeric_var_who=sum(sapply(complete_full_bank[,1:21],is.numeric))
numeric_var_who
# Checking for number of columns with character type
char_var_who=sum(sapply(complete_full_bank[,1:21],is.character))
char_var_who
# Checking for column names with numeric type
numeric_varname_who=which(sapply(complete_full_bank[,1:21],is.numeric))
numeric_varname_who
# Checking for column names with character type
char_varname_who=which(sapply(complete_full_bank[,1:21],is.character))
char_varname_who
```

### Is our Response Variables Unbalanced?

```{r is-data-balanced}
prop.table(table(complete_full_bank$y))
freq(complete_full_bank, input="y")

plot_bar(data=complete_full_bank, by = "y", nrow=2, ncol=2)
```

Looking closer at our response variable, we have a significant lower percentage of yes's versus no's (12.66% yes's to 87.34% no's). With this in mind, we'll need to ensure either our sampling is weighted, or consider algorithms that can oversample or undersample when fitting our logistic regression model, otherwise we may introduce bias and lower our accuracy on predicting when a client successfully subscribes to a bank term deposit.

Additionally, we do not have any representation of those who have defaulted on their credit and if they choose not to subscribe to a bank term deposit. This variable could pose some issues when analyzing how defaults affect a client's desire to subscribe. Later we will look further into this and see if this predictor more closely when creating our models.

### Examining Normality of Numeric Variables
```{r normality-numeric}
plot_num(complete_full_bank)
```
Looking closer at the histograms of the scale variables, it appears there are some potential normality issues, particularly the pdays.

## Initial Full Logistic Regression Model
Before we go into automatic model variable selection tools, we should attempt to understand the full model and how our explanatory variables may come into play and assess whether the assumptions of logistic regression are met or not.

```{r full-model}
full.logistic.fit <- glm(y ~ ., data = complete_full_bank, family="binomial")
summary(full.logistic.fit) # Full Model Statistics

psuedo_rsq(full.logistic.fit) # Psuedo R Statistic w/ Chi-Sq P-value

library(ROCit)

full.measures <- measureit(score = full.logistic.fit$fitted.values, class = full.logistic.fit$y,
                           measure = c("ACC", "SENS", "FSCR"))

plot(full.measures$ACC~full.measures$Cutoff, type = "l")
roc_empirical <- rocit(score = full.logistic.fit$fitted.values, class = full.logistic.fit$y, negref = "0")

plot(roc_empirical, values=F)
```
### Interpreting the coefficients
The following variables are not useful predictors as they contain moderate to large p-values depending on the confidence level we seek to achieve:
* age w/ p-value of 0.42
* Jobs:
  + entrepreneur -- p-value of 0.22
  + maid -- p-value of 0.81
  + management -- p-value of 0.82
  + employed -- p-value of 0.52
  + technician -- p-value of 0.63
  + unemployed -- p-value of 0.82
* Martial statuses of married (p-value of .96) and single (p-value of .75)
* Education Levels:
  + basic of 6 years -- p-value of .42
  + basic of 9 years -- p-value of .82
  + high school -- p-value of .40
  + illiterate -- p-value of .06
  + professional course -- p-value of 0.24
* Defaulting on Credit (p-values of .94)
* Housing Loan (p-value of .64)
* A loan in general (p-value of .36)
* Last Month of Contact During Campaign:
  + July -- p-value of .66
  + October -- p-value of  .14
  + December -- p-value of .31
* Day of the Week of Contact During Campaign:
  + Monday -- p-value of .35
  + Thursday -- p-value of.08
* The Number of Previous Contacts Performed Before the Campaign -- p-value of .41

Additionally, we can observe that for our full model, our residual deviance is 13,902 with an AIC of 13,998. We can also see that our psuedo R^2 value is 0.39975 with an extremely small Chi-squared p-value that it is nonetheless 0.

Lastly, we can see how the full model predicted by the following graphic:
```{r full-model-s-line}
s_plot(full.logistic.fit, complete_full_bank)
```
As you can see, there looks to be some scattered mis-classifications within the yes section. This is perhaps due to unbalanced data that we need to sort out in our refined model with sampling methods as previously mentioned.

## Assumptions Investigation

Now that we understand more about our data and logistic regression model, we should do some preliminary assumptions check for logistic regression. These metrics are important as we will use them to compare our later simpler logistic regression model for performance evaluation.

### Binary/Ordinal Response Variable
```{r response-variable-assumption}
nlevels(complete_full_bank$y)
levels(complete_full_bank$y)
```
As you can see above, our response variable, y, consists of two levels, resulting in a binary response. We will move forward with a binary logistic regression model instead of an ordinal logistic regression model. 

### Influential Observations
```{r residuals}
plot(full.logistic.fit, which = 4, id.n = 3)
```
As we can see from our Cook's Distance visualization, it appears we may have 3 observations of influence between 0.08 and 0.012. However, despite these are small values, we will elect to remove them as an extra precaution.

```{r remove-influential}
complete_full_bank_2 <- complete_full_bank[-c(18463,27544,27535)]
```


### Linearity of Continuous/Scale Variables vs. Log Odds of Response Variable

```{r linearity-assumption}
numeric_pred <- complete_full_bank %>% select_if(is.numeric)
numeric_pred_names <- colnames(numeric_pred)

# binding logit and numeric predictors for scatterplots
linearity_data <- numeric_pred %>%
  mutate(logit = log(full.logistic.fit$fitted.values/(1-full.logistic.fit$fitted.values))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(linearity_data, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess", formula = "y~x") +
  theme_bw() +
  facet_wrap(~predictors, scales = "free_y")
```
We can observe from our plot that a number of our variables do not follow a linear trend. However, we do see that duration and potentially, campaign, have a linear trend associated with subscribing to a bank term deposit.

For all other variables, transformations may be beneficial for classifying, but for now we will leave them as is. 

### Multicollinearity of Explanatory Variables

The below correlation table shows the correlation between the numerical variables:
* nr.employed and emp.var.rate are 91% correlated. 
* nr.employed and euribor3m are 95% correlated.
* emp.var.rate and euribor3m are 97% correlated.
* cons.price.idx and emp.var.rate are 78% correlated.
* cons.price.idx and euribor3m are 69% correlated.
* cons.price.idx and nr.employed are 52% correlated.

Furthermore, we do have a moderate multicollinearity issue with some variables for previous and pdays. We may want to examine them closer.

```{r Correlation}
ggcorr(data=numeric_pred, label = T, nbreaks=5, label_size = 3, hjust = 0.9, size = 3, layout.exp = 4) +
  labs(title = "Multicollinearity of Variables (Pairwise / Pearson's correlation)")
```
For variables with moderate to large Pearson's correlation, we'll need to examine the VIF to see which variables may be need to be removed due to a problematic amount of collinearity.
```{r VIF}
vif(full.logistic.fit)
```
As a rule of thumb, we should pay attention to variables with a VIF higher than 5 or 10, but we'll use 5 for logistic regression. The following variables are showing significant VIF values:
* job
* month
* pdays
* poutcome
* emp.var.rate
* cons.price.idx
* cons.conf.idx
* euribor3m
* nr.employed

With our earlier analysis on the coefficients, we can see why some predictors were not as significant or useful (e.g. jobs, month, etc.). To refine our model further, we'll remove the above variables.

## PCA Examination
```{r PCA}
numeric_preds_bank <- complete_full_bank %>% keep(is.numeric)
my.cor <- cor(numeric_preds_bank)
library(gplots)
library(ggplot2)
heatmap.2(my.cor,
          col=redgreen(75), 
          density.info="none", 
          trace="none", 
          dendrogram=c("row"), 
          symm=F,
          symkey=T,
          symbreaks=T,
          scale="none", 
          srtCol = 35)
#Another option here would be to do PCA among the continous predictors to see
#if they seperate out.  Or a heatmap.
pc.result<-prcomp(numeric_preds_bank %>% keep(is.numeric),scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-complete_full_bank$y
#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of y")
ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of y")
ggplot(data = pc.scores, aes(x = PC3, y = PC4)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of y")
```
There does not seem to be a large separation with PCA.

## Second Logistic Regression Model
```{r lr-model-2}
logistic.fit.2 <- glm(y ~ . -nr.employed -euribor3m -cons.conf.idx -cons.price.idx -emp.var.rate -poutcome -pdays -month -job, data = complete_full_bank, family="binomial")

summary(logistic.fit.2)
psuedo_rsq(logistic.fit.2)
s_plot(logistic.fit.2, complete_full_bank_2)
```
After adjusting the model for multicollinearity problems, we can now see that our model has worsened in performance. However, it does seem that our s-plot is starting to form when compared with the initial full model.

## Model Selection
As we can see, manually modifying and tweaking a model can have adverse results. With this we'll introduced a third and final model to do a comparison with using automatic variable selection method, LASSO, for this, we'll need to create a sampling of test data to train the model and then a test set to predict with. We'll use a 80:20 split and balance our data.

```{r data-split}
set.seed(2008)

training_samples <- complete_full_bank_2$y %>% createDataPartition(p=0.8, list = F)

train.data <- complete_full_bank[training_samples,]
test.data <- complete_full_bank[-training_samples,]

# Create matrix of predictors & convert to categorical predictors to appropriate dummy values
## Dummy code categorical predictor variables
x <- model.matrix(y~., train.data)[,-1]
## Convert outcome/class to numerical variable
y <- ifelse(train.data$y == "no", 1, 0)

# Source on stepping through Penalized Logistic Regression: http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/#compute-lasso-regression
```

### LASSO
```{r LASSO Lambda}
# Set seed and find ideal lambda for LASSO
set.seed(2008)
cv.l.model <- cv.glmnet(x, y, family = "binomial", alpha = 1) # Remeber alpha=1 means LASSO Regression

# Plot of ideal lambda for minimizing CV error
plot(cv.l.model)

# Comparing Regression Coefficients from CV of lambda
coefplot(cv.l.model, lambda=cv.l.model$lambda.min, family="binomial")
coef(cv.l.model, cv.l.model$lambda.min)

coef(cv.l.model, cv.l.model$lambda.1se)
coefplot(cv.l.model, lambda=cv.l.model$lambda.1se, family="binomial")

# Graphic to interact with lambda and coeficients
coefpath(cv.l.model)
```
As we do not have an analyst to help specify the lambda value to use in our LASSO model to control the coefficient shrinkage, we elected to use cross-validation error to find a suitable lambda for our data. As can be observed, when examining the cross-validation error according to the log of lambda, our left dashed vertical line indicates the optimal value of -6.

When choosing which lambda value to use when fitting our model, we generally want a balance between accuracy but also simplicity. That way, we can easily interpret the model if need be. Looking closer, we can examine from the coefficient tables, which lambda will provide a simple model. In this case, the within 1 standard error (1se) lambda has 25 variables that have non-zero coefficients, while our minimum lambda has 7 non-zero coefficients. For our initial model, we will use within 1 standard error lambda to produce a simpler model for understanding rather than for accuracy.

```{R LASSO-model}
set.seed(2008)

# Fit a model w/ ideal lambda from cross-validation
l.model <- glmnet(x, y, alpha = 1, family="binomial", lambda = cv.l.model$lambda.1se)

# Predict on test data
x.test <- model.matrix(y~., test.data)[,-1]
probabilities <- l.model %>% predict(newx = x.test, type="response")
predicted.classes <- ifelse(probabilities > 0.5, "no", "yes")

# Accuracy Rate
observed.classes <- test.data$y
mean(predicted.classes == observed.classes)
```