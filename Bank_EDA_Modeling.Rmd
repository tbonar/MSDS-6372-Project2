---
title: "Bank Analysis And Modeling"
author: "Taylor Bonar & Michael Burgess & Rashmi Patel"
date: "7/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse) # Data handling
library(naniar) # Viz on Missing Data
library(GGally) # Graphs!
library(funModeling) # Helpful Functions for EDA Function
library(Hmisc) # Helpful Functions for EDA Function
library(caret) # Data Partitioning
library(glmnet) # Modeling
library(coefplot) # Coefficient modeling of glmnet objects

setwd(".")
basic_eda <- function(data) # Sample Function Source: https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}
```
# Introduction
The goal of this paper is to investigate banking data via an exploratory data analysis. Once we have initially examined the data, we will then move forward with attempting a classification model via logistic regression for predicting whether or not a client will subscribe to a banking institution given a direct marketing campaign taking place.

# Exploratory Data Analysis

The data we'll be exploring is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) relating to the direct marketing campaigns of a Portuguese banking institution. The initial data set consists of 45,211 observations with 20 inputs, dating between May 2008 to November 2010.

As described by the UCI Machine Learning Repository, each of the variables/columns are described as:

>**Input variables:**
>**bank client data:**
>
>1 - age (numeric)
>
>2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
>
>3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
>
>4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
>
>5 - default: has credit in default? (categorical: 'no','yes','unknown')
>
>6 - housing: has housing loan? (categorical: 'no','yes','unknown')
>
>7 - loan: has personal loan? (categorical: 'no','yes','unknown')
>
>**related with the last contact of the current campaign:**
>
>8 - contact: contact communication type (categorical: 'cellular','telephone')
>
>9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
>
>10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
>
>11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
>
>**other attributes:**
>
>12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
>
>13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
>
>14 - previous: number of contacts performed before this campaign and for this client (numeric)
>
>15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
>
># social and economic context attributes
>
>16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
>
>17 - cons.price.idx: consumer price index - monthly indicator (numeric)
>
>18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
>
>19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
>
>20 - nr.employed: number of employees - quarterly indicator (numeric)
>
>**Output variable (desired target):**
>
>21 - y - has the client subscribed a term deposit? (binary: 'yes','no')

Another caution we will need to access is that within this dataset, there is potential for more than one contact to the same client. This repeat contact was necessary as it was required to assess the product (i.e. a bank term deposit) and whether the client would or would not be subscribed.

```{r EDA}
# Retrieve datasets zip
if(!file.exists("./data/bank.zip")) {
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", "./bank-additional.zip", mode="wb")
}
unzip("./bank-additional.zip", files = c("bank-additional/bank-additional.csv","bank-additional/bank-additional-full.csv","bank-additional/bank-additional-names.txt"))

# Read data into a data frame object
full_bank <- read.csv("./bank-additional/bank-additional-full.csv", header = T, sep = ";")

# Use function to create initial data insights for bank data
basic_eda(full_bank)

# Create a bird's eye view of missing data using naniar library if missing data exists
if(sum(!complete.cases(full_bank)) > 0)
{
  vis_miss(full_bank, cluster = F) + # Without aggregating observations
  labs(title = "NAs in Bank Data from May 2008 - Nov 2010") +
  theme(axis.text.x = element_text(angle=90))
}

# Transform all chr objects in data frame to a factor class as a secondary data frame object
full_bank_2 <- as.data.frame(unclass(full_bank), stringsAsFactors = T)
```

# Logistic Regression Modeling
As we have examined the model above, we are looking to see if with a direct marketing campaign, via phone calls, will allow us to predict whether or not a client will subscribe for a term deposit. Before we begin, let's have an initial look at our data and see if we can use logistic regression for prediction classification.

## Model Selection
Before we check the assumptions for logistic regression, we will use several means of automatic model selection to see what out of the 21 variables may be more useful for our logistic regression model. Once we've reduced our variables, we can then proceed forward with checking the assumptions. For our models, we'll use a 80/20 split of the data for predictions.

```{r data-split}
set.seed(2008)

training_samples <- full_bank_2$y %>% createDataPartition(p=0.8, list = F)

train.data <- full_bank_2[training_samples,]
test.data <- full_bank_2[-training_samples,]

# Create matrix of predictors & convert to categorical predictors to appropriate dummy values
## Dummy code categorical predictor variables
x <- model.matrix(y~., train.data)[,-1]
## Convert outcome/class to numerical variable
y <- ifelse(train.data$y == "no", 1, 0)

# Source on stepping through Penalized Logistic Regression: http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/#compute-lasso-regression
```

### LASSO
```{r LASSO logistic model}
# Set seed and find ideal lambda for LASSO
set.seed(2008)
cv.l.model <- cv.glmnet(x, y, family = "binomial", alpha = 1) # Remeber alpha=1 means LASSO Regression

# Plot of ideal lambda for minimizing CV error
plot(cv.l.model)

# Comparing Regression Coefficients from CV of lambda
coefplot(cv.l.model, lambda=cv.l.model$lambda.min, family="binomial")
coef(cv.l.model, cv.l.model$lambda.min)

coef(cv.l.model, cv.l.model$lambda.1se)
coefplot(cv.l.model, lambda=cv.l.model$lambda.1se, family="binomial")

# Graphic to interact with lambda and coeficients
coefpath(cv.l.model)

# Fit a model w/ ideal lambda from cross-validation
l.model <- glmnet(x, y, alpha = 1, family="binomial", lambda = cv.l.model$lambda.1se)
```
As we do not have an analyst to help specify the lambda value to use in our LASSO model to control the coefficient shrinkage, we elected to use cross-validation error to find a suitable lambda for our data. As can be observed, when examining the cross-validation error according to the log of lambda, our left dashed vertical line indicates the optimal value of -6.

When choosing which lambda value to use when fitting our model, we generally want a balance between accuracy but also simplicity. That way, we can easily interpret the model if need be. Looking closer, we can examine from the coefficient tables, which lambda will provide a simple model. In this case, the within 1 standard error (1se) lambda has 25 variables that have non-zero coefficients, while our minimum lambda has 7 non-zero coefficients. For our initial model, we will use within 1 standard error lambda to produce a simpler model for understanding rather than for accuracy.

## Assumptions Investigation
```{r logistic regression assumptions}

```

## Interpreting the Parameters


## Conclusion



